{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b62d5bc",
   "metadata": {},
   "source": [
    "# KNN Imputaion and FinalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc360cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Raw Data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/1_Raw/\"\n",
    "raw_data = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \".pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b71bd",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e526095",
   "metadata": {},
   "source": [
    "# KNN Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3ee59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [02:24, 510.26it/s]\n",
      "88557it [03:05, 477.88it/s]\n",
      "42510it [01:32, 461.61it/s]\n",
      "51261it [01:58, 433.51it/s]\n",
      "75365it [02:43, 460.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inter Column and Between Instance Imputation\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_2_BaselineImputation/\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "k = 100\n",
    "number_of_partitions = 5\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((60,25,np.array(raw_data[i]).shape[2]))\n",
    "    new_partition = np.array(raw_data[i])\n",
    "    \n",
    "    with tqdm(np.array(raw_data[i]).shape[2]) as pbar:\n",
    "        for j in range(0,np.array(raw_data[i]).shape[2]):\n",
    "            new_column = np.zeros((60,25))  \n",
    "            new_column = new_partition[:,:,j]\n",
    "            for m in range(0,24):\n",
    "                new_column[:,m+1][new_column[:,m+1] == 0] = np.nan\n",
    "                \n",
    "                if np.isnan(new_column[:,m+1]).all():\n",
    "                    new_column[:,m+1] = np.zeros(60) \n",
    "                            \n",
    "                else:\n",
    "                    new_2d = new_column[:,m+1].reshape(len(new_column[:,m+1]), 1)\n",
    "                    new_column[:,m+1] = imputer.fit_transform(new_2d)[:,0]\n",
    "                 \n",
    "                \n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)\n",
    "            \n",
    "            \n",
    "# Between Instance Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96646f2",
   "metadata": {},
   "source": [
    "# Mean Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9796ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:11, 6664.22it/s]\n",
      "88557it [00:12, 7065.21it/s]\n",
      "42510it [00:06, 7001.24it/s]\n",
      "51261it [00:07, 7023.04it/s]\n",
      "75365it [00:10, 6959.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inter Column and Between Instance Imputation\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_2_BaselineImputation/\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "k = 100\n",
    "number_of_partitions = 5\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((60,25,np.array(raw_data[i]).shape[2]))\n",
    "    new_partition = np.array(raw_data[i])\n",
    "    \n",
    "    with tqdm(np.array(raw_data[i]).shape[2]) as pbar:\n",
    "        for j in range(0,np.array(raw_data[i]).shape[2]):\n",
    "            new_column = np.zeros((60,25))  \n",
    "            new_column = new_partition[:,:,j]\n",
    "            for m in range(0,24):\n",
    "                \n",
    "                if np.isnan(new_column[:,m+1]).all():\n",
    "                    new_column[:,m+1] = np.zeros(60) \n",
    "                else:\n",
    "                    mean_non_zero = np.mean(new_column[:,m+1][new_column[:,m+1] != 0])\n",
    "                    new_column[:,m+1][new_column[:,m+1] == 0] = mean_non_zero\n",
    "                 \n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_MeanImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)\n",
    "            \n",
    "            \n",
    "# Between Instance Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474308",
   "metadata": {},
   "source": [
    "# similarity-based KNN Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf9aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Function\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def mvts_pearson_correlation(X, Y):\n",
    "    if X.shape != Y.shape:\n",
    "        raise ValueError(\"Input arrays X and Y must have the same shape.\")\n",
    "\n",
    "    # Calculate the Pearson correlation coefficient between X and Y\n",
    "    X = X.T.flatten()\n",
    "    Y = Y.T.flatten()\n",
    "    try:\n",
    "        correlation_coefficient , p = pearsonr(X, Y)\n",
    "        if np.isnan(correlation_coefficient):\n",
    "            correlation_coefficient = -1.0\n",
    "    except:\n",
    "        correlation_coefficient = -1.0\n",
    "    \n",
    "    return correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f04790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [14:03, 87.11it/s] \n",
      "88557it [17:51, 82.66it/s] \n",
      "42510it [08:15, 85.86it/s] \n",
      "51261it [10:58, 77.81it/s]\n",
      "75365it [16:19, 76.97it/s] \n"
     ]
    }
   ],
   "source": [
    "# Inter Column and Between Instance Imputation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "k_count = 50\n",
    "k = 50\n",
    "number_of_partitions = 5\n",
    "num_attributes = 25\n",
    "num_timestamps = 60\n",
    "\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((num_timestamps,num_attributes,np.array(raw_data[i]).shape[2]))\n",
    "    new_partition = np.array(raw_data[i])\n",
    "    \n",
    "    with tqdm(new_partition.shape[2]) as pbar:\n",
    "        for j in range(0,new_partition.shape[2]):\n",
    "            new_column = np.zeros((num_timestamps,num_attributes))  \n",
    "            new_column = new_partition[:,:,j]\n",
    "            \n",
    "            \n",
    "            \n",
    "            new_column[new_column == 0.] = np.nan\n",
    "            \n",
    "            if np.isnan(new_column[:,1:25]).all():\n",
    "                new_column = new_partition[:,:,j-1]\n",
    "            \n",
    "            nan_index = []\n",
    "            if np.isnan(new_column).any():\n",
    "                for m in range(0,num_attributes-1):\n",
    "                    if np.isnan(new_column[:,m+1]).any():\n",
    "                        nan_index.append(m+1)\n",
    "                \n",
    "                if j < k_count:\n",
    "                    if j == 0:\n",
    "                        k = 1\n",
    "                    else:\n",
    "                        k = j\n",
    "                else:\n",
    "                    k = k_count\n",
    "                correlation_coefficient = np.full(k, -2.0)\n",
    "                the_X = new_column\n",
    "                \n",
    "                the_X = np.nan_to_num(the_X, nan=0.0)\n",
    "                \n",
    "                for n in range(0, k):\n",
    "                    the_Y = new_partition[:,:,j-n-1]\n",
    "                    correlation_coefficient[n] = mvts_pearson_correlation(the_X[:,1:25], the_Y[:,1:25])\n",
    "                    \n",
    "                for m in range(0,num_attributes-1):\n",
    "\n",
    "                    if np.isnan(new_column[:,m+1]).all():\n",
    "                        \n",
    "                        indices_of_largest = np.where(correlation_coefficient == np.max(correlation_coefficient))\n",
    "                        first_occurrence_index = indices_of_largest[0][0]\n",
    "                        new_column[:,m+1] = new_partition[:,m+1,j-first_occurrence_index-1]\n",
    "\n",
    "                    else:\n",
    "                        if j>=2:\n",
    "\n",
    "                            sorted_indices = np.argsort(correlation_coefficient)[::-1]  # Sort in descending order\n",
    "                            largest_index = sorted_indices[0]  # Index of the largest item\n",
    "                            second_largest_index = sorted_indices[1]\n",
    "\n",
    "                            new_2d = [new_partition[:,m+1,j-second_largest_index-1], new_partition[:,m+1,j-largest_index-1], new_column[:,m+1]]\n",
    "                            new_column[:,m+1] = imputer.fit_transform(new_2d)[2,:]\n",
    "                        else:\n",
    "                            new_2d = new_column[:,m+1].reshape(-1, 1)\n",
    "                            new_column[:,m+1] = imputer.fit_transform(new_2d)[:,0]\n",
    "                \n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)\n",
    "            \n",
    "            \n",
    "# Between Instance Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e4c51",
   "metadata": {},
   "source": [
    "# Missing Value Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ff544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \"_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d53c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_values(data, start_partition, end_partition):\n",
    "    abt_header = ['Timestamp', 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                               'USFLUX','TOTFZ','MEANPOT', 'EPSX', 'EPSY','EPSZ','MEANSHR','SHRGT45','MEANGAM',\n",
    "                                  'MEANGBT','MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "    num_columns = 25\n",
    "    num_timestamps = 60\n",
    "    num_partitions = 5\n",
    "    null_count = [0,0,0,0,0]\n",
    "    non_null_count = [0,0,0,0,0]\n",
    "    null_count_per_feature = np.zeros((num_partitions,num_columns), dtype=int)\n",
    "\n",
    "    for i in range(start_partition-1, end_partition):\n",
    "        partition = np.array(data[i])\n",
    "\n",
    "        for j in range(0,partition.shape[2]):\n",
    "            mvts = partition[:,:, j]\n",
    "            for m in range(0,num_columns):\n",
    "                for n in range (0,num_timestamps):\n",
    "                    if (mvts[n,m] == 0.0 or np.isnan(mvts[n,m]) or np.isinf(mvts[n,m])):\n",
    "                        null_count[i] += 1\n",
    "                        null_count_per_feature[i,m] += 1\n",
    "                    else:\n",
    "                        non_null_count[i] += 1\n",
    "\n",
    "        print(\"Partition\" + str(i+1) + \":\")\n",
    "        print(\"null counts in P\" + str(i+1) + \": \" + str(null_count[i]))\n",
    "        print(\"non-null counts in P\"+ str(i+1) + \": \" + str(non_null_count[i]))\n",
    "        for x in range(0,num_columns):\n",
    "            print(abt_header[x] + \": \" + str(null_count_per_feature[i,x]))\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ab200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition1:\n",
      "null counts in P1: 0\n",
      "non-null counts in P1: 110238000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition2:\n",
      "null counts in P2: 0\n",
      "non-null counts in P2: 132835500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition3:\n",
      "null counts in P3: 0\n",
      "non-null counts in P3: 63765000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition4:\n",
      "null counts in P4: 0\n",
      "non-null counts in P4: 76891500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition5:\n",
      "null counts in P5: 0\n",
      "non-null counts in P5: 113047500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_missing_values(imputed_data,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec6e16",
   "metadata": {},
   "source": [
    "# Final Data With MVTS-KNNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5109f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \"_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0435e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = []\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/2_Labels/\"\n",
    "\n",
    "for i in range(1,6):\n",
    "    labels.append(pd.read_csv(data_dir + \"Partition\" + str(i) + \"_labels.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3adbf5",
   "metadata": {},
   "source": [
    "## Concatenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a709c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation with Shuffle\n",
    "\n",
    "def multi_to_uni(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], num_timestamps*(num_attributes-1)))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "\n",
    "                flettened = np.zeros(num_timestamps*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    flettened[(m-1)*num_timestamps:m*num_timestamps] = new_column[:,m]\n",
    "\n",
    "                new_partition[j,:] = flettened\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))\n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Concatenation_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_Concatenation_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657a835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:01, 41704.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [00:02, 37354.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [00:01, 42142.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [00:01, 43855.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [00:01, 43659.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_1_FinalData_Concatenation_KnnImputation/\"\n",
    "\n",
    "multi_to_uni(1, 5, data_dir, imputed_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e16c7",
   "metadata": {},
   "source": [
    "## New Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35482ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewFeatures with Shuffle\n",
    "\n",
    "def new_Features_pkl(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "\n",
    "    number_of_new_features = 9\n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], number_of_new_features*24))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "                \n",
    "\n",
    "                new_features = np.zeros(number_of_new_features*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    \n",
    "                    mean = np.mean(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 0] = mean\n",
    "                    median = np.median(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 1] = median\n",
    "                    std = np.std(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 2] = std\n",
    "                    \n",
    "                    skewness = skew(new_column[:,m])\n",
    "                    if skewness == np.nan:\n",
    "                        skewness = new_partition[j-1, ((m-1)*number_of_new_features) + 3]\n",
    "                    new_features[((m-1)*number_of_new_features) + 3] = skewness\n",
    "                    \n",
    "                    kurtosis_value = kurtosis(new_column[:,m])\n",
    "                    if kurtosis_value == np.nan:\n",
    "                        kurtosis_value = new_partition[j-1, ((m-1)*number_of_new_features) + 4]\n",
    "                    new_features[((m-1)*number_of_new_features) + 4] = kurtosis_value\n",
    "                    \n",
    "                    indices = np.arange(num_timestamps)\n",
    "                    weight_array = indices / num_timestamps\n",
    "                    weighted_avg = np.average(new_column[:,m], weights=weight_array)\n",
    "                    if weighted_avg == np.nan:\n",
    "                        weighted_avg = new_partition[j-1, ((m-1)*number_of_new_features) + 5]\n",
    "                    new_features[((m-1)*number_of_new_features) + 5] = weighted_avg\n",
    "                    \n",
    "                    last_value = new_column[59,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 6] = last_value\n",
    "                    first_value = new_column[0,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 7] = first_value\n",
    "                    \n",
    "                    numerator = np.sum((new_column[:,m] - mean) * (indices - np.mean(indices)))\n",
    "                    denominator = np.sum((new_column[:,m] - mean) ** 2)\n",
    "                    slope = numerator / denominator\n",
    "                    if slope == np.nan:\n",
    "                        slope = new_partition[j-1, ((m-1)*number_of_new_features) + 8]\n",
    "                    new_features[((m-1)*number_of_new_features) + 8] = slope\n",
    "                \n",
    "                    \n",
    "                new_partition[j,:] = new_features\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))  \n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_NewFeatures_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_NewFeatures_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c502ffb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imputed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m      7\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_2_FinalData_NewFeatures_KnnImputation/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m new_Features_pkl(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, data_dir, imputed_data, labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imputed_data' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_2_FinalData_NewFeatures_KnnImputation/\"\n",
    "\n",
    "new_Features_pkl(1, 5, data_dir, imputed_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dab87",
   "metadata": {},
   "source": [
    "## 3DPKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb5426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D pickle with shuffle\n",
    "import pickle\n",
    "\n",
    "def data_for_sequencemodels(start_partition, end_partition, data_dir, data, labels):\n",
    "    \n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    sequence_length = 60\n",
    "    num_features = 25\n",
    "\n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        num_samples = np.array(data[i]).shape[2]\n",
    "        X_train = np.zeros((num_samples, sequence_length, num_features-1))\n",
    "        Y_train = np.zeros(num_samples)\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        each_partition = np.zeros((sequence_length, num_features, num_samples))\n",
    "        each_partition = np.array(data[i])\n",
    "\n",
    "        with tqdm(num_samples) as pbar:\n",
    "            for j in range(0, num_samples):\n",
    "\n",
    "                X_train[j, :, :] = each_partition[:,1:num_features,j]\n",
    "                Y_train[j] = the_labels['FLARE_CLASS'].iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train).any()))            \n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_3DPKL_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_3DPKL_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_3_FinalData_3DPKL_KnnImputation/\"\n",
    "\n",
    "data_for_sequencemodels(1, 5, data_dir, imputed_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f5fc7",
   "metadata": {},
   "source": [
    "# Final Data Without Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a58a7",
   "metadata": {},
   "source": [
    "## Concatenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37db2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation with Shuffle\n",
    "\n",
    "def multi_to_uni(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], num_timestamps*(num_attributes-1)))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "                \n",
    "                new_column = np.nan_to_num(new_column, nan=0.0)\n",
    "                \n",
    "                flettened = np.zeros(num_timestamps*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    flettened[(m-1)*num_timestamps:m*num_timestamps] = new_column[:,m]\n",
    "\n",
    "                new_partition[j,:] = flettened\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))\n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Concatenation_Raw\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_Concatenation_Raw\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03152a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:02, 28942.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [00:03, 28486.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [00:01, 27675.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [00:01, 28415.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [00:02, 26911.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_6_FinalData_Concatenation_Raw/\"\n",
    "\n",
    "multi_to_uni(1, 5, data_dir, raw_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cb867",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f4c85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewFeatures with Shuffle\n",
    "\n",
    "def new_Features_pkl(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "\n",
    "    number_of_new_features = 9\n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], number_of_new_features*24))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "                \n",
    "                new_column = np.nan_to_num(new_column, nan=0.0)\n",
    "\n",
    "                new_features = np.zeros(number_of_new_features*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    all_zeros = np.all(new_column[:,m] == 0.0)\n",
    "                    if all_zeros:\n",
    "                        new_features[((m-1)*number_of_new_features):((m-1)*number_of_new_features)+9] = 0\n",
    "                    else:\n",
    "                        mean = np.mean(new_column[:,m])\n",
    "                        new_features[((m-1)*number_of_new_features) + 0] = mean\n",
    "                        median = np.median(new_column[:,m])\n",
    "                        new_features[((m-1)*number_of_new_features) + 1] = median\n",
    "                        std = np.std(new_column[:,m])\n",
    "                        new_features[((m-1)*number_of_new_features) + 2] = std\n",
    "\n",
    "                        skewness = skew(new_column[:,m])\n",
    "                        if np.isreal(skewness) == False:\n",
    "                            skewness = new_partition[j-1, ((m-1)*number_of_new_features) + 3]\n",
    "                        new_features[((m-1)*number_of_new_features) + 3] = skewness\n",
    "\n",
    "                        kurtosis_value = kurtosis(new_column[:,m])\n",
    "                        if np.isreal(kurtosis_value) == False:\n",
    "                            kurtosis_value = new_partition[j-1, ((m-1)*number_of_new_features) + 4]\n",
    "                        new_features[((m-1)*number_of_new_features) + 4] = kurtosis_value\n",
    "\n",
    "                        indices = np.arange(num_timestamps)\n",
    "                        weight_array = indices / num_timestamps\n",
    "                        weighted_avg = np.average(new_column[:,m], weights=weight_array)\n",
    "                        if weighted_avg == np.nan:\n",
    "                            weighted_avg = new_partition[j-1, ((m-1)*number_of_new_features) + 5]\n",
    "                        new_features[((m-1)*number_of_new_features) + 5] = weighted_avg\n",
    "\n",
    "                        last_value = new_column[59,m]\n",
    "                        new_features[((m-1)*number_of_new_features) + 6] = last_value\n",
    "                        first_value = new_column[0,m]\n",
    "                        new_features[((m-1)*number_of_new_features) + 7] = first_value\n",
    "\n",
    "                        numerator = np.sum((new_column[:,m] - mean) * (indices - np.mean(indices)))\n",
    "                        denominator = np.sum((new_column[:,m] - mean) ** 2)\n",
    "                        slope = numerator / denominator\n",
    "                        if np.isreal(slope) == False:\n",
    "                            slope = new_partition[j-1, ((m-1)*number_of_new_features) + 8]\n",
    "                        new_features[((m-1)*number_of_new_features) + 8] = slope\n",
    "                \n",
    "                new_partition[j,:] = new_features\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))  \n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_NewFeatures_Raw\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_NewFeatures_Raw\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8400bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [07:42, 158.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [09:06, 162.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [04:25, 160.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [05:19, 160.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [07:49, 160.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_7_FinalData_NewFeatures_Raw/\"\n",
    "\n",
    "new_Features_pkl(1, 5, data_dir, raw_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244eb710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
