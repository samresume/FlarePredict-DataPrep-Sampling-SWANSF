{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b62d5bc",
   "metadata": {},
   "source": [
    "# KNN Imputaion and FinalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc360cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Raw Data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/1_Raw/\"\n",
    "raw_data = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \".pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b71bd",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474308",
   "metadata": {},
   "source": [
    "# KNN Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf9aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance Function\n",
    "def mvts_pearson_correlation(X, Y):\n",
    "    if X.shape != Y.shape:\n",
    "        raise ValueError(\"Input arrays X and Y must have the same shape.\")\n",
    "\n",
    "    # Calculate the Pearson correlation coefficient between X and Y\n",
    "    correlation_coefficient = -1\n",
    "    try:\n",
    "        correlation_coefficient = np.corrcoef(X.T, Y.T)[0, X.shape[1]]\n",
    "    except:\n",
    "        correlation_coefficient = -1\n",
    "\n",
    "    return correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f04790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [08:39, 141.48it/s]\n",
      "88557it [25:48, 57.17it/s] \n",
      "42510it [05:34, 127.08it/s]\n",
      "51261it [06:55, 123.38it/s]\n",
      "75365it [12:59, 96.71it/s] \n"
     ]
    }
   ],
   "source": [
    "# Inter Column and Between Instance Imputation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "k1 = 100\n",
    "k2 = 10\n",
    "number_of_partitions = 5\n",
    "num_attributes = 25\n",
    "num_timestamps = 60\n",
    "\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((num_timestamps,num_attributes,np.array(raw_data[i]).shape[2]))\n",
    "    new_partition = np.array(raw_data[i])\n",
    "    \n",
    "    with tqdm(new_partition.shape[2]) as pbar:\n",
    "        for j in range(0,new_partition.shape[2]):\n",
    "            new_column = np.zeros((num_timestamps,num_attributes))  \n",
    "            new_column = new_partition[:,:,j]\n",
    "            \n",
    "            for m in range(0,num_attributes-1):\n",
    "                # we will start from 1 since we do not need to work on timestamps which are the first columns (0)\n",
    "                new_column[:,m+1][new_column[:,m+1] == 0.0] = np.nan\n",
    "                \n",
    "                if np.isnan(new_column[:,m+1]).all():\n",
    "                    correlation_coefficient = np.full(k1, -2)\n",
    "                    if j < k1:\n",
    "                        k1 = j\n",
    "                    for n in range(0, k1):\n",
    "                        if np.isnan(new_partition[:,m+1,j-n-1]).any() == False:\n",
    "                            the_X = np.delete(new_partition[:,:,j], m+1, axis=1)\n",
    "                            the_Y = np.delete(new_partition[:,:,j-n-1], m+1, axis=1)\n",
    "                            correlation_coefficient[n] = mvts_pearson_correlation(the_X, the_Y)\n",
    "            \n",
    "                    indices_of_largest = np.where(correlation_coefficient == np.max(correlation_coefficient))\n",
    "                    first_occurrence_index = indices_of_largest[0][0]\n",
    "                    new_column[:,m+1] = new_partition[:,m+1,j-first_occurrence_index-1]\n",
    "\n",
    "                else:\n",
    "                    if j>=2:\n",
    "                        correlation_coefficient = np.full(k2, -2)\n",
    "                        if j < k2:\n",
    "                            k2 = j\n",
    "                        for n in range(0, k2):\n",
    "                            the_X = np.delete(new_partition[:,:,j], m+1, axis=1)\n",
    "                            the_Y = np.delete(new_partition[:,:,j-n-1], m+1, axis=1)\n",
    "                            correlation_coefficient[n] = mvts_pearson_correlation(the_X, the_Y)                            \n",
    "                        sorted_indices = np.argsort(correlation_coefficient)[::-1]  # Sort in descending order\n",
    "                        largest_index = sorted_indices[0]  # Index of the largest item\n",
    "                        second_largest_index = sorted_indices[1]\n",
    "                        \n",
    "                        new_2d = [new_partition[:,m+1,j-second_largest_index-1], new_partition[:,m+1,j-largest_index-1], new_column[:,m+1]]\n",
    "                        new_column[:,m+1] = imputer.fit_transform(new_2d)[2,:]\n",
    "                    else:\n",
    "                        new_2d = new_column[:,m+1].reshape(-1, 1)\n",
    "                        new_column[:,m+1] = imputer.fit_transform(new_2d)[:,0]\n",
    "                \n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)\n",
    "            \n",
    "            \n",
    "# Between Instance Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e4c51",
   "metadata": {},
   "source": [
    "# Missing Value Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ff544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \"_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d53c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_values(data, start_partition, end_partition):\n",
    "    abt_header = ['Timestamp', 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                               'USFLUX','TOTFZ','MEANPOT', 'EPSX', 'EPSY','EPSZ','MEANSHR','SHRGT45','MEANGAM',\n",
    "                                  'MEANGBT','MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "    num_columns = 25\n",
    "    num_timestamps = 60\n",
    "    num_partitions = 5\n",
    "    null_count = [0,0,0,0,0]\n",
    "    non_null_count = [0,0,0,0,0]\n",
    "    null_count_per_feature = np.zeros((num_partitions,num_columns), dtype=int)\n",
    "\n",
    "    for i in range(start_partition-1, end_partition):\n",
    "        partition = np.array(data[i])\n",
    "\n",
    "        for j in range(0,partition.shape[2]):\n",
    "            mvts = partition[:,:, j]\n",
    "            for m in range(0,num_columns):\n",
    "                for n in range (0,num_timestamps):\n",
    "                    if (mvts[n,m] == 0.0 or np.isnan(mvts[n,m]).any() or np.isinf(mvts[n,m]).any()):\n",
    "                        null_count[i] += 1\n",
    "                        null_count_per_feature[i,m] += 1\n",
    "                    else:\n",
    "                        non_null_count[i] += 1\n",
    "\n",
    "        print(\"Partition\" + str(i+1) + \":\")\n",
    "        print(\"null counts in P\" + str(i+1) + \": \" + str(null_count[i]))\n",
    "        print(\"non-null counts in P\"+ str(i+1) + \": \" + str(non_null_count[i]))\n",
    "        for x in range(0,num_columns):\n",
    "            print(abt_header[x] + \": \" + str(null_count_per_feature[i,x]))\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ab200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition1:\n",
      "null counts in P1: 0\n",
      "non-null counts in P1: 110238000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition2:\n",
      "null counts in P2: 0\n",
      "non-null counts in P2: 132835500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition3:\n",
      "null counts in P3: 0\n",
      "non-null counts in P3: 63765000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition4:\n",
      "null counts in P4: 0\n",
      "non-null counts in P4: 76891500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition5:\n",
      "null counts in P5: 0\n",
      "non-null counts in P5: 113047500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_missing_values(imputed_data,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec6e16",
   "metadata": {},
   "source": [
    "# Final Data (Concatenation, New Features, 3DPKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5109f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \"_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0435e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = []\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/2_Labels/\"\n",
    "\n",
    "for i in range(1,6):\n",
    "    labels.append(pd.read_csv(data_dir + \"Partition\" + str(i) + \"_labels.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3adbf5",
   "metadata": {},
   "source": [
    "## Concatenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a709c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation with Shuffle\n",
    "\n",
    "def multi_to_uni(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], num_timestamps*(num_attributes-1)))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "\n",
    "                flettened = np.zeros(num_timestamps*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    flettened[(m-1)*num_timestamps:m*num_timestamps] = new_column[:,m]\n",
    "\n",
    "                new_partition[j,:] = flettened\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))\n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Concatenation_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_Concatenation_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657a835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:01, 48785.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [00:02, 43769.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [00:00, 43167.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [00:01, 44764.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [00:01, 45059.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_1_FinalData_Concatenation_KnnImputation/\"\n",
    "\n",
    "multi_to_uni(1, 5, data_dir, imputed_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e16c7",
   "metadata": {},
   "source": [
    "## New Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35482ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewFeatures with Shuffle\n",
    "\n",
    "def new_Features_pkl(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "\n",
    "    number_of_new_features = 9\n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], number_of_new_features*24))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "                \n",
    "\n",
    "                new_features = np.zeros(number_of_new_features*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    \n",
    "                    mean = np.mean(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 0] = mean\n",
    "                    median = np.median(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 1] = median\n",
    "                    std = np.std(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 2] = std\n",
    "                    \n",
    "                    skewness = skew(new_column[:,m])\n",
    "                    if skewness == np.nan:\n",
    "                        skewness = new_partition[j-1, ((m-1)*number_of_new_features) + 3]\n",
    "                    new_features[((m-1)*number_of_new_features) + 3] = skewness\n",
    "                    \n",
    "                    kurtosis_value = kurtosis(new_column[:,m])\n",
    "                    if kurtosis_value == np.nan:\n",
    "                        kurtosis_value = new_partition[j-1, ((m-1)*number_of_new_features) + 4]\n",
    "                    new_features[((m-1)*number_of_new_features) + 4] = kurtosis_value\n",
    "                    \n",
    "                    indices = np.arange(num_timestamps)\n",
    "                    weight_array = indices / num_timestamps\n",
    "                    weighted_avg = np.average(new_column[:,m], weights=weight_array)\n",
    "                    if weighted_avg == np.nan:\n",
    "                        weighted_avg = new_partition[j-1, ((m-1)*number_of_new_features) + 5]\n",
    "                    new_features[((m-1)*number_of_new_features) + 5] = weighted_avg\n",
    "                    \n",
    "                    last_value = new_column[59,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 6] = last_value\n",
    "                    first_value = new_column[0,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 7] = first_value\n",
    "                    \n",
    "                    numerator = np.sum((new_column[:,m] - mean) * (indices - np.mean(indices)))\n",
    "                    denominator = np.sum((new_column[:,m] - mean) ** 2)\n",
    "                    slope = numerator / denominator\n",
    "                    if slope == np.nan:\n",
    "                        slope = new_partition[j-1, ((m-1)*number_of_new_features) + 8]\n",
    "                    new_features[((m-1)*number_of_new_features) + 8] = slope\n",
    "                \n",
    "                    \n",
    "                new_partition[j,:] = new_features\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))  \n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_NewFeatures_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_NewFeatures_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c502ffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [07:45, 158.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [09:26, 156.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [04:31, 156.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [05:28, 156.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [08:01, 156.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_2_FinalData_NewFeatures_KnnImputation/\"\n",
    "\n",
    "new_Features_pkl(1, 5, data_dir, imputed_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dab87",
   "metadata": {},
   "source": [
    "## 3DPKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb5426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D pickle with shuffle\n",
    "import pickle\n",
    "\n",
    "def data_for_sequencemodels(start_partition, end_partition, data_dir, data, labels):\n",
    "    \n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    sequence_length = 60\n",
    "    num_features = 25\n",
    "\n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        num_samples = np.array(data[i]).shape[2]\n",
    "        X_train = np.zeros((num_samples, sequence_length, num_features-1))\n",
    "        Y_train = np.zeros(num_samples)\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        each_partition = np.zeros((sequence_length, num_features, num_samples))\n",
    "        each_partition = np.array(data[i])\n",
    "\n",
    "        with tqdm(num_samples) as pbar:\n",
    "            for j in range(0, num_samples):\n",
    "\n",
    "                X_train[j, :, :] = each_partition[:,1:num_features,j]\n",
    "                Y_train[j] = the_labels['FLARE_CLASS'].iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train).any()))            \n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_3DPKL_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_3DPKL_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a1f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:00, 151957.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [00:00, 138900.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [00:00, 126583.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [00:00, 137563.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [00:00, 137814.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_3_FinalData_3DPKL_KnnImputation/\"\n",
    "\n",
    "data_for_sequencemodels(1, 5, data_dir, imputed_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
