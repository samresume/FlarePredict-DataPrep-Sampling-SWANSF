{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5581c1f4",
   "metadata": {},
   "source": [
    "# Preparing Final Data for ML and DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca025e9",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96d3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Raw Data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_2_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "normalized_data = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'rb') as f:\n",
    "        normalized_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef69cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = []\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/2_Labels/\"\n",
    "\n",
    "for i in range(1,6):\n",
    "    labels.append(pd.read_csv(data_dir + \"Partition\" + str(i) + \"_labels.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226a7cd",
   "metadata": {},
   "source": [
    "## CSV Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36745511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation with Shuffle\n",
    "\n",
    "def multi_to_uni(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], num_timestamps*(num_attributes-1)))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "\n",
    "                flettened = np.zeros(num_timestamps*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    flettened[(m-1)*num_timestamps:m*num_timestamps] = new_column[:,m]\n",
    "\n",
    "                new_partition[j,:] = flettened\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))\n",
    "        X_train = pd.DataFrame(new_partition)\n",
    "        Y_train = pd.DataFrame(new_partition_label)\n",
    "\n",
    "\n",
    "        num_samples = len(X_train)\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "        X_train_shuffled = X_train.iloc[shuffle_indices].reset_index(drop=True)\n",
    "        Y_train_shuffled = Y_train.iloc[shuffle_indices].reset_index(drop=True)\n",
    "\n",
    "        Y_train_shuffled = Y_train_shuffled.rename(columns={0: 'Flare_Class'})\n",
    "\n",
    "\n",
    "        X_train_shuffled.to_csv(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Concatenation_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".csv\", index=False)\n",
    "\n",
    "        Y_train_shuffled.to_csv(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_Concatenation_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0214e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:01, 43171.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [00:02, 43207.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [00:01, 41830.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [00:01, 43332.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [00:01, 42613.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_1_FinalDataset_ML_Concatenation_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "\n",
    "multi_to_uni(1, 5, data_dir, normalized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b254021",
   "metadata": {},
   "source": [
    "## CSV With New Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eeb3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewFeatures with Shuffle\n",
    "\n",
    "def new_Features(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "\n",
    "    number_of_new_features = 9\n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], number_of_new_features*24))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "                \n",
    "\n",
    "                new_features = np.zeros(number_of_new_features*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    \n",
    "                    mean = np.mean(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 0] = mean\n",
    "                    median = np.median(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 1] = median\n",
    "                    std = np.std(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 2] = std\n",
    "                    \n",
    "                    skewness = skew(new_column[:,m])\n",
    "                    if skewness == np.nan:\n",
    "                        skewness = new_partition[j-1, ((m-1)*number_of_new_features) + 3]\n",
    "                    new_features[((m-1)*number_of_new_features) + 3] = skewness\n",
    "                    \n",
    "                    kurtosis_value = kurtosis(new_column[:,m])\n",
    "                    if kurtosis_value == np.nan:\n",
    "                        kurtosis_value = new_partition[j-1, ((m-1)*number_of_new_features) + 4]\n",
    "                    new_features[((m-1)*number_of_new_features) + 4] = kurtosis_value\n",
    "                    \n",
    "                    indices = np.arange(num_timestamps)\n",
    "                    weight_array = indices / num_timestamps\n",
    "                    weighted_avg = np.average(new_column[:,m], weights=weight_array)\n",
    "                    if weighted_avg == np.nan:\n",
    "                        weighted_avg = new_partition[j-1, ((m-1)*number_of_new_features) + 5]\n",
    "                    new_features[((m-1)*number_of_new_features) + 5] = weighted_avg\n",
    "                    \n",
    "                    last_value = new_column[59,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 6] = last_value\n",
    "                    first_value = new_column[0,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 7] = first_value\n",
    "                    \n",
    "                    numerator = np.sum((new_column[:,m] - mean) * (indices - np.mean(indices)))\n",
    "                    denominator = np.sum((new_column[:,m] - mean) ** 2)\n",
    "                    slope = numerator / denominator\n",
    "                    if slope == np.nan:\n",
    "                        slope = new_partition[j-1, ((m-1)*number_of_new_features) + 8]\n",
    "                    new_features[((m-1)*number_of_new_features) + 8] = slope\n",
    "                \n",
    "                new_partition[j,:] = new_features\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "                        \n",
    "        for z in range(0,num_attributes-1):\n",
    "\n",
    "            data_2d = new_partition[:,z*9+3].reshape(-1, 1)\n",
    "            new_partition[:,z*9+3] = scaler.fit_transform(data_2d).flatten()\n",
    "\n",
    "            data_2d = new_partition[:,z*9+4].reshape(-1, 1)\n",
    "            new_partition[:,z*9+4] = scaler.fit_transform(data_2d).flatten()\n",
    "\n",
    "            data_2d = new_partition[:,z*9+8].reshape(-1, 1)\n",
    "            new_partition[:,z*9+8] = scaler.fit_transform(data_2d).flatten()\n",
    "        \n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))  \n",
    "        X_train = pd.DataFrame(new_partition)\n",
    "        Y_train = pd.DataFrame(new_partition_label)\n",
    "        \n",
    "        num_samples = len(X_train)\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "        X_train_shuffled = X_train.iloc[shuffle_indices].reset_index(drop=True)\n",
    "        Y_train_shuffled = Y_train.iloc[shuffle_indices].reset_index(drop=True)\n",
    "\n",
    "        Y_train_shuffled = Y_train_shuffled.rename(columns={0: 'Flare_Class'})\n",
    "\n",
    "\n",
    "        X_train_shuffled.to_csv(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_NewFeatures_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".csv\", index=False)\n",
    "\n",
    "        Y_train_shuffled.to_csv(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_NewFeatures_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a14ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [08:02, 152.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [09:50, 149.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [04:41, 151.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [05:39, 150.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10374it [01:09, 151.88it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_2_FinalTrain&Test_ML_NewFeatures_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "\n",
    "new_Features(1, 5, data_dir, normalized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ce8b5",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48714068",
   "metadata": {},
   "source": [
    "## 3D PKL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b287e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D pickle with shuffle\n",
    "import pickle\n",
    "\n",
    "def data_for_sequencemodels(start_partition, end_partition, data_dir, data, labels):\n",
    "    \n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "    \n",
    "    sequence_length = 60\n",
    "    num_features = 25\n",
    "\n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        num_samples = np.array(data[i]).shape[2]\n",
    "        X_train = np.zeros((num_samples, sequence_length, num_features-1))\n",
    "        Y_train = np.zeros(num_samples)\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        each_partition = np.zeros((sequence_length, num_features, num_samples))\n",
    "        each_partition = np.array(data[i])\n",
    "\n",
    "        with tqdm(num_samples) as pbar:\n",
    "            for j in range(0, num_samples):\n",
    "\n",
    "                X_train[j, :, :] = each_partition[:,1:num_features,j]\n",
    "                Y_train[j] = the_labels['FLARE_CLASS'].iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(X_train).any()))            \n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_3DPKL_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_3DPKL_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868a89c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [00:00, 144528.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [00:00, 132273.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [00:00, 125149.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [00:00, 135427.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [00:00, 135515.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_3_FinalDataset_DL_3DPKL_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "\n",
    "data_for_sequencemodels(1, 5, data_dir, normalized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed6af1",
   "metadata": {},
   "source": [
    "## 2D PKL With New Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea38a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewFeatures with Shuffle\n",
    "\n",
    "def new_Features_pkl(start_partition, end_partition, data_dir, data, labels):\n",
    "    category_mapping = {'X': 1, 'M': 1, 'B': 0, 'C': 0, 'FQ': 0}\n",
    "\n",
    "    number_of_new_features = 9\n",
    "    num_attributes = 25\n",
    "    num_timestamps = 60\n",
    "    \n",
    "    for i in range(start_partition-1,end_partition):\n",
    "        \n",
    "        the_labels = pd.DataFrame()\n",
    "        the_labels['FLARE_CLASS'] = labels[i]['FLARE_CLASS'].map(category_mapping)\n",
    "        new_partition = np.zeros((np.array(data[i]).shape[2], number_of_new_features*24))\n",
    "        new_partition_label = np.zeros(new_partition.shape[0])\n",
    "        \n",
    "        each_partition = np.zeros((num_timestamps, num_attributes, new_partition.shape[0]))\n",
    "        each_partition = np.array(data[i])\n",
    "        \n",
    "        with tqdm(new_partition.shape[0]) as pbar:\n",
    "            for j in range(0,new_partition.shape[0]):\n",
    "                new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "                new_column = each_partition[:,:,j]\n",
    "                \n",
    "\n",
    "                new_features = np.zeros(number_of_new_features*(num_attributes-1))\n",
    "\n",
    "                for m in range(1,num_attributes):\n",
    "                    \n",
    "                    mean = np.mean(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 0] = mean\n",
    "                    median = np.median(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 1] = median\n",
    "                    std = np.std(new_column[:,m])\n",
    "                    new_features[((m-1)*number_of_new_features) + 2] = std\n",
    "                    \n",
    "                    skewness = skew(new_column[:,m])\n",
    "                    if skewness == np.nan:\n",
    "                        skewness = new_partition[j-1, ((m-1)*number_of_new_features) + 3]\n",
    "                    new_features[((m-1)*number_of_new_features) + 3] = skewness\n",
    "                    \n",
    "                    kurtosis_value = kurtosis(new_column[:,m])\n",
    "                    if kurtosis_value == np.nan:\n",
    "                        kurtosis_value = new_partition[j-1, ((m-1)*number_of_new_features) + 4]\n",
    "                    new_features[((m-1)*number_of_new_features) + 4] = kurtosis_value\n",
    "                    \n",
    "                    indices = np.arange(num_timestamps)\n",
    "                    weight_array = indices / num_timestamps\n",
    "                    weighted_avg = np.average(new_column[:,m], weights=weight_array)\n",
    "                    if weighted_avg == np.nan:\n",
    "                        weighted_avg = new_partition[j-1, ((m-1)*number_of_new_features) + 5]\n",
    "                    new_features[((m-1)*number_of_new_features) + 5] = weighted_avg\n",
    "                    \n",
    "                    last_value = new_column[59,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 6] = last_value\n",
    "                    first_value = new_column[0,m]\n",
    "                    new_features[((m-1)*number_of_new_features) + 7] = first_value\n",
    "                    \n",
    "                    numerator = np.sum((new_column[:,m] - mean) * (indices - np.mean(indices)))\n",
    "                    denominator = np.sum((new_column[:,m] - mean) ** 2)\n",
    "                    slope = numerator / denominator\n",
    "                    if slope == np.nan:\n",
    "                        slope = new_partition[j-1, ((m-1)*number_of_new_features) + 8]\n",
    "                    new_features[((m-1)*number_of_new_features) + 8] = slope\n",
    "                \n",
    "                    \n",
    "                new_partition[j,:] = new_features\n",
    "                new_partition_label[j] = the_labels.iloc[j]\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "        for z in range(0,num_attributes-1):\n",
    "\n",
    "            data_2d = new_partition[:,z*9+3].reshape(-1, 1)\n",
    "            new_partition[:,z*9+3] = scaler.fit_transform(data_2d).flatten()\n",
    "\n",
    "            data_2d = new_partition[:,z*9+4].reshape(-1, 1)\n",
    "            new_partition[:,z*9+4] = scaler.fit_transform(data_2d).flatten()\n",
    "\n",
    "            data_2d = new_partition[:,z*9+8].reshape(-1, 1)\n",
    "            new_partition[:,z*9+8] = scaler.fit_transform(data_2d).flatten()\n",
    "\n",
    "        print(\"P\"+str(i+1)+\" Nan-Value: \"+ str(np.isnan(new_partition).any()))  \n",
    "        X_train = new_partition\n",
    "        Y_train = new_partition_label\n",
    "\n",
    "\n",
    "        num_samples = X_train.shape[0]\n",
    "        shuffle_indices = np.random.permutation(num_samples)\n",
    "\n",
    "        X_train_shuffled = X_train[shuffle_indices]\n",
    "        Y_train_shuffled = Y_train[shuffle_indices]\n",
    "    \n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_2DPKL_NewFeatures_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(X_train_shuffled, f)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) \n",
    "                       + \"_Labels_2DPKL_NewFeatures_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(Y_train_shuffled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fa1fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [07:38, 160.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88557it [09:19, 158.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42510it [04:30, 157.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51261it [05:22, 158.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P4 Nan-Value: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75365it [08:13, 152.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P5 Nan-Value: False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/5_4_FinalDataset_DL_2DPKL_NewFeatures_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "\n",
    "new_Features_pkl(1, 5, data_dir, normalized_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d03a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
