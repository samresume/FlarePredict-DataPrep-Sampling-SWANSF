{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b62d5bc",
   "metadata": {},
   "source": [
    "# Normalization and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc360cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Raw Data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/1_Raw/\"\n",
    "raw_data = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \".pkl\", 'rb') as f:\n",
    "        raw_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b71bd",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3474308",
   "metadata": {},
   "source": [
    "# KNN Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f04790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [03:53, 314.29it/s]\n",
      "88557it [08:55, 165.31it/s]\n",
      "42510it [02:33, 276.31it/s]\n",
      "51261it [03:51, 221.76it/s]\n",
      "75365it [04:58, 252.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inter Column and Between Instance Imputation\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=6)\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "k = 100\n",
    "number_of_partitions = 5\n",
    "num_attributes = 25\n",
    "num_timestamps = 60\n",
    "\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((num_timestamps,num_attributes,np.array(raw_data[i]).shape[2]))\n",
    "    new_partition = np.array(raw_data[i])\n",
    "    \n",
    "    with tqdm(new_partition.shape[2]) as pbar:\n",
    "        for j in range(0,new_partition.shape[2]):\n",
    "            new_column = np.zeros((num_timestamps,num_attributes))  \n",
    "            new_column = new_partition[:,:,j]\n",
    "            \n",
    "            for m in range(0,num_attributes-1):\n",
    "                # we will start from 1 since we do not need to work on timestamps which are the first columns (0)\n",
    "                new_column[:,m+1][new_column[:,m+1] == 0.0] = np.nan\n",
    "                \n",
    "                if np.isnan(new_column[:,m+1]).all():\n",
    "                    for n in range(0, k):\n",
    "                        if np.isnan(new_partition[:,m+1,j-n-1]).any() == False:\n",
    "                            new_column[:,m+1] = new_partition[:,m+1,j-n-1]\n",
    "                            break\n",
    "                else:\n",
    "                    if j>1: \n",
    "                        new_2d = [new_partition[:,m+1,j-2], new_partition[:,m+1,j-1], new_column[:,m+1]]\n",
    "                        new_column[:,m+1] = imputer.fit_transform(new_2d)[2,:]\n",
    "                    else:\n",
    "                        new_2d = new_column[:,m+1].reshape(-1, 1)\n",
    "                        new_column[:,m+1] = imputer.fit_transform(new_2d)[:,0]\n",
    "                \n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)\n",
    "            \n",
    "            \n",
    "# Between Instance Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e4c51",
   "metadata": {},
   "source": [
    "# Missing Value Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ff544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \"_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d53c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_values(data, start_partition, end_partition):\n",
    "    abt_header = ['Timestamp', 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                               'USFLUX','TOTFZ','MEANPOT', 'EPSX', 'EPSY','EPSZ','MEANSHR','SHRGT45','MEANGAM',\n",
    "                                  'MEANGBT','MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "    num_columns = 25\n",
    "    num_timestamps = 60\n",
    "    num_partitions = 5\n",
    "    null_count = [0,0,0,0,0]\n",
    "    non_null_count = [0,0,0,0,0]\n",
    "    null_count_per_feature = np.zeros((num_partitions,num_columns), dtype=int)\n",
    "\n",
    "    for i in range(start_partition-1, end_partition):\n",
    "        partition = np.array(data[i])\n",
    "\n",
    "        for j in range(0,partition.shape[2]):\n",
    "            mvts = partition[:,:, j]\n",
    "            for m in range(0,num_columns):\n",
    "                for n in range (0,num_timestamps):\n",
    "                    if (mvts[n,m] == 0.0 or np.isnan(mvts[n,m]).any()):\n",
    "                        null_count[i] += 1\n",
    "                        null_count_per_feature[i,m] += 1\n",
    "                    else:\n",
    "                        non_null_count[i] += 1\n",
    "\n",
    "        print(\"Partition\" + str(i+1) + \":\")\n",
    "        print(\"null counts in P\" + str(i+1) + \": \" + str(null_count[i]))\n",
    "        print(\"non-null counts in P\"+ str(i+1) + \": \" + str(non_null_count[i]))\n",
    "        for x in range(0,num_columns):\n",
    "            print(abt_header[x] + \": \" + str(null_count_per_feature[i,x]))\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ab200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition1:\n",
      "null counts in P1: 0\n",
      "non-null counts in P1: 110238000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition2:\n",
      "null counts in P2: 0\n",
      "non-null counts in P2: 132835500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition3:\n",
      "null counts in P3: 0\n",
      "non-null counts in P3: 63765000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition4:\n",
      "null counts in P4: 0\n",
      "non-null counts in P4: 76891500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition5:\n",
      "null counts in P5: 0\n",
      "non-null counts in P5: 113047500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_missing_values(imputed_data,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e47a2",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0dcca",
   "metadata": {},
   "source": [
    "# Zscore and MinMax Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5dc64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [02:51, 427.29it/s]\n",
      "88557it [03:53, 379.53it/s]\n",
      "42510it [01:51, 381.03it/s]\n",
      "51261it [02:16, 375.96it/s]\n",
      "33695it [01:29, 367.62it/s]"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_1_KnnImputation_ZscoreMinMaxNormalization/\"\n",
    "\n",
    "number_of_partitions = 5\n",
    "num_attributes = 25\n",
    "num_timestamps = 60\n",
    "\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((num_timestamps,num_attributes,np.array(imputed_data[i]).shape[2]))\n",
    "    new_partition = np.array(imputed_data[i])\n",
    "    \n",
    "    with tqdm(new_partition.shape[2]) as pbar:\n",
    "        for j in range(0,new_partition.shape[2]):\n",
    "            new_column = np.zeros((num_timestamps,num_attributes))  \n",
    "            new_column = new_partition[:,:,j]\n",
    "            \n",
    "            for m in range(0,num_attributes-1):\n",
    "                if np.std(new_column[:,m+1])== 0.0:\n",
    "                    minmax = np.ones(60)\n",
    "                else:\n",
    "                    zscore = stats.zscore(new_column[:,m+1])\n",
    "                    data_2d = zscore.reshape(-1, 1)\n",
    "                    minmax = scaler.fit_transform(data_2d).flatten()\n",
    "                if (np.isnan(minmax).any()):\n",
    "                    print('nan-zscore')\n",
    "                    \n",
    "                new_column[:,m+1] = minmax\n",
    "                \n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation_ZScoreMinMaxNormalization\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)\n",
    "\n",
    "        \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5ed2a",
   "metadata": {},
   "source": [
    "# Log, Square, BoxCox, Zscore, and MinMax Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41777da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73492it [07:31, 162.69it/s]\n",
      "88557it [09:59, 147.65it/s]\n",
      "42510it [04:45, 148.99it/s]\n",
      "51261it [05:52, 145.43it/s]\n",
      "75365it [08:44, 143.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_2_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "number_of_partitions = 5\n",
    "num_attributes = 25\n",
    "num_timestamps = 60\n",
    "\n",
    "for i in range(0,number_of_partitions):\n",
    "    new_partition = np.zeros((num_timestamps,num_attributes,np.array(imputed_data[i]).shape[2]))\n",
    "    new_partition = np.array(imputed_data[i])\n",
    "    \n",
    "    with tqdm(new_partition.shape[2]) as pbar:\n",
    "        for j in range(0,new_partition.shape[2]):\n",
    "            new_column = np.zeros((num_timestamps,num_attributes)) \n",
    "            new_column = new_partition[:,:,j]\n",
    "            minmax = np.zeros(num_timestamps)\n",
    "            all_positive = np.zeros(num_timestamps)\n",
    "            \n",
    "            for m in range(0,num_attributes-1):\n",
    "                the_min = np.min(new_column[:,m+1])\n",
    "                the_max = np.max(new_column[:,m+1])\n",
    "                skewness = stats.skew(new_column[:,m+1])\n",
    "                \n",
    "                if (the_max - the_min > 10000):\n",
    "                    if (skewness > 1):\n",
    "                        if (the_min < 0):\n",
    "                            all_positive = new_column[:,m+1] + abs(the_min) + 0.1\n",
    "                        else:\n",
    "                            all_positive = new_column[:,m+1]\n",
    "                        if np.std(all_positive)== 0.0:\n",
    "                            minmax = np.ones(60)\n",
    "                        else:\n",
    "                            log = np.log(all_positive)\n",
    "                            zscore = stats.zscore(log)\n",
    "                            data_2d = zscore.reshape(-1, 1)\n",
    "                            minmax = scaler.fit_transform(data_2d).flatten()\n",
    "                        if (np.isnan(minmax).any()):\n",
    "                            print('nan-log')\n",
    "\n",
    "                    elif (skewness < -1):\n",
    "                        if (the_min < 0):\n",
    "                            all_positive = new_column[:,m+1] + abs(the_min) + 0.1\n",
    "                        else:\n",
    "                            all_positive = new_column[:,m+1]\n",
    "\n",
    "                        if np.std(all_positive)== 0.0:\n",
    "                            minmax = np.ones(60)\n",
    "                        else:\n",
    "                            sqrt = np.sqrt(all_positive)\n",
    "                            zscore = stats.zscore(sqrt)\n",
    "                            data_2d = zscore.reshape(-1, 1)\n",
    "                            minmax = scaler.fit_transform(data_2d).flatten()\n",
    "                        if (np.isnan(minmax).any()):\n",
    "                            print('nan-sqrt')\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        if np.std(new_column[:,m+1])== 0.0:\n",
    "                            minmax = np.ones(60)\n",
    "                        else:\n",
    "                            zscore = stats.zscore(new_column[:,m+1])\n",
    "                            data_2d = zscore.reshape(-1, 1)\n",
    "                            minmax = scaler.fit_transform(data_2d).flatten()\n",
    "                        if (np.isnan(minmax).any()):\n",
    "                            print('nan-zscore')\n",
    "                        \n",
    "\n",
    "                else:\n",
    "                    if (skewness > 1 or skewness < -1):\n",
    "\n",
    "                        if (the_min < 0):\n",
    "                            all_positive = new_column[:,m+1] + abs(the_min) + 0.1\n",
    "                        else:\n",
    "                            all_positive = new_column[:,m+1]\n",
    "\n",
    "                        \n",
    "                        if np.std(all_positive)== 0.0:\n",
    "                            minmax = np.ones(60)\n",
    "                        else:\n",
    "                            boxcox, values = stats.boxcox(all_positive)\n",
    "                            minmax = stats.zscore(boxcox)\n",
    "                            data_2d = zscore.reshape(-1, 1)\n",
    "                            minmax = scaler.fit_transform(data_2d).flatten()\n",
    "                        if (np.isnan(minmax).any()):\n",
    "                            print('nan-boxcox')\n",
    "                    else:\n",
    "\n",
    "                        if np.std(new_column[:,m+1])== 0.0:\n",
    "                            minmax = np.ones(60)\n",
    "                        else:\n",
    "                            zscore = stats.zscore(new_column[:,m+1])\n",
    "                            data_2d = zscore.reshape(-1, 1)\n",
    "                            minmax = scaler.fit_transform(data_2d).flatten()\n",
    "                        if (np.isnan(minmax).any()):\n",
    "                            print('nan-zscore')\n",
    "\n",
    "                new_column[:,m+1] = minmax\n",
    "                \n",
    "            new_partition[:,:,j] = new_column\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'wb') as f:\n",
    "            pickle.dump(new_partition, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d69055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Raw Data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/4_2_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization/\"\n",
    "normalized_data = []\n",
    "\n",
    "num_partitions = 5\n",
    "\n",
    "for i in range(0,num_partitions):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i+1) + \"_KnnImputation_LogSquareBoxCoxZscoreMinMaxNormalization\" + \".pkl\", 'rb') as f:\n",
    "        normalized_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe6dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_values_normalized(data, start_partition, end_partition):\n",
    "    abt_header = ['Timestamp', 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                               'USFLUX','TOTFZ','MEANPOT', 'EPSX', 'EPSY','EPSZ','MEANSHR','SHRGT45','MEANGAM',\n",
    "                                  'MEANGBT','MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "    num_columns = 25\n",
    "    num_timestamps = 60\n",
    "    num_partitions = 5\n",
    "    null_count = [0,0,0,0,0]\n",
    "    non_null_count = [0,0,0,0,0]\n",
    "    null_count_per_feature = np.zeros((num_partitions,num_columns), dtype=int)\n",
    "\n",
    "    for i in range(start_partition-1, end_partition):\n",
    "        partition = np.array(data[i])\n",
    "\n",
    "        for j in range(0,partition.shape[2]):\n",
    "            mvts = partition[:,:, j]\n",
    "            for m in range(0,num_columns):\n",
    "                for n in range (0,num_timestamps):\n",
    "                    if (np.isnan(mvts[n,m]).any()):\n",
    "                        null_count[i] += 1\n",
    "                        null_count_per_feature[i,m] += 1\n",
    "                    else:\n",
    "                        non_null_count[i] += 1\n",
    "\n",
    "        print(\"Partition\" + str(i+1) + \":\")\n",
    "        print(\"null counts in P\" + str(i+1) + \": \" + str(null_count[i]))\n",
    "        print(\"non-null counts in P\"+ str(i+1) + \": \" + str(non_null_count[i]))\n",
    "        for x in range(0,num_columns):\n",
    "            print(abt_header[x] + \": \" + str(null_count_per_feature[i,x]))\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8cfae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition1:\n",
      "null counts in P1: 0\n",
      "non-null counts in P1: 110238000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition2:\n",
      "null counts in P2: 0\n",
      "non-null counts in P2: 132835500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition3:\n",
      "null counts in P3: 0\n",
      "non-null counts in P3: 63765000\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition4:\n",
      "null counts in P4: 0\n",
      "non-null counts in P4: 76891500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n",
      "Partition5:\n",
      "null counts in P5: 0\n",
      "non-null counts in P5: 113047500\n",
      "Timestamp: 0\n",
      "R_VALUE: 0\n",
      "TOTUSJH: 0\n",
      "TOTBSQ: 0\n",
      "TOTPOT: 0\n",
      "TOTUSJZ: 0\n",
      "ABSNJZH: 0\n",
      "SAVNCPP: 0\n",
      "USFLUX: 0\n",
      "TOTFZ: 0\n",
      "MEANPOT: 0\n",
      "EPSX: 0\n",
      "EPSY: 0\n",
      "EPSZ: 0\n",
      "MEANSHR: 0\n",
      "SHRGT45: 0\n",
      "MEANGAM: 0\n",
      "MEANGBT: 0\n",
      "MEANGBZ: 0\n",
      "MEANGBH: 0\n",
      "MEANJZH: 0\n",
      "TOTFY: 0\n",
      "MEANJZD: 0\n",
      "MEANALP: 0\n",
      "TOTFX: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_missing_values_normalized(normalized_data,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc796e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
