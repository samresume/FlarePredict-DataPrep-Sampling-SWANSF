{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2800dcd",
   "metadata": {},
   "source": [
    "# Result of Knn Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496d3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/3_KnnImputation/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \"_KnnImputation\" + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d858bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = []\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/2_Labels/\"\n",
    "\n",
    "for i in range(1,6):\n",
    "    labels.append(pd.read_csv(data_dir + \"Partition\" + str(i) + \"_labels.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6816e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTW Similarity: -0.3046459148442567\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define your multivariate time series sequences\n",
    "time_series1 = np.array([\n",
    "    2.0, 8.0, 2.1, 3.1, 2.2, 3.2\n",
    "    \n",
    "    # Add more data points as needed\n",
    "])\n",
    "\n",
    "time_series2 = np.array([\n",
    "    3.0, 4.0, 6.1, 4.1, 9.2, 4.2\n",
    "    \n",
    "    # Add more data points as needed\n",
    "])\n",
    "\n",
    "# Calculate DTW similarity\n",
    "similarity , p =stats.pearsonr(time_series1, time_series2)\n",
    "\n",
    "print(f\"DTW Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a7ea8",
   "metadata": {},
   "source": [
    "## Without KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/1_Raw/\"\n",
    "imputed_data = []\n",
    "\n",
    "number_of_partitions = 5\n",
    "for i in range(1,number_of_partitions +1):\n",
    "# Load the array with Pickle\n",
    "    with open(data_dir + \"Partition\" + str(i) + \".pkl\", 'rb') as f:\n",
    "        imputed_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d4ab5",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a729a",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c600cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float(\"{:.2f}\".format(13.949999999999999))\n",
    "\n",
    "def TSS(TP,TN,FP,FN):\n",
    "    TSS_value = (TP / (TP + FN)) - (FP / (FP + TN))\n",
    "    return TSS_value\n",
    "\n",
    "def HSS1(TP,TN,FP,FN):\n",
    "    HSS1_value = (2 * (TP * TN - FP * FN)) / ((TP + FN) * (FN + TN) + (TP + FP) * (FP + TN))\n",
    "    return HSS1_value\n",
    "    \n",
    "def HSS2(TP,TN,FP,FN):\n",
    "    HSS2_value = (2 * (TP * TN - FP * FN)) / ((TP + FP) * (FN + TN) + (TP + FN) * (FP + TN))\n",
    "    return HSS2_value\n",
    "\n",
    "def GSS(TP,TN,FP,FN):\n",
    "    GSS_value = (TP - (TP + FP) * (TP + FN) / (TP + FP + FN + TN))\n",
    "    return GSS_value\n",
    "\n",
    "def Recall(TP,TN,FP,FN):\n",
    "    Recall_value = (TP) / (TP + FN)\n",
    "    return Recall_value\n",
    "\n",
    "def FPR(TP,TN,FP,FN):\n",
    "    fpr_value = (FP) / (FP + TN)\n",
    "    return fpr_value\n",
    "\n",
    "def Accuracy(TP,TN,FP,FN):\n",
    "    accuracy_value = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return accuracy_value\n",
    "\n",
    "def Precision(TP,TN,FP,FN):\n",
    "    precision_value = (FP) / (TP + FP)\n",
    "    return precision_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893ea43",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bda8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(name, X_train, Y_train, training_func, num, rocket_kernels= 1500, tsf_estimator=25):\n",
    "    kfold = np.array([[1,2],[1,3],[1,4],[1,5],[2,3],[2,4],[2,5],[3,4],[3,5],[4,5]])\n",
    "    metrics = []\n",
    "    metrics_values = np.array([])\n",
    "    \n",
    "    for i in range(0, num):\n",
    "        train_index = kfold[i,0]\n",
    "        test_index = kfold[i,1]\n",
    "        metrics_values = training_func(X_train[train_index-1], Y_train[train_index-1], X_train[test_index-1], Y_train[test_index-1])\n",
    "\n",
    "        metrics.append(np.append(np.append(train_index, test_index), metrics_values))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189dd05",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ae446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def gru_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "    \n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features = 60, 24\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=120, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=keras.metrics.SpecificityAtSensitivity(sensitivity=0.95))\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    threshold = 0.40  # Adjust the threshold as needed\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    print(str(X_train.shape)+': GRU Classifier is Done! \\n')\n",
    "\n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"mlp_model.pkl\")\n",
    "\n",
    "    #loaded_mlp_model = joblib.load(data_dir + \"mlp_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886ac3c",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa217f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def svm_model(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/models/\"\n",
    "\n",
    "\n",
    "    # Create an SVM classifier (you can choose different kernels like 'linear', 'rbf', etc.)\n",
    "    svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "    svm_classifier.fit(X_train, Y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(str(X_train.shape)+': SVM Classifier is Done! \\n')\n",
    "    \n",
    "\n",
    "    confusion = confusion_matrix(Y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    tss = TSS(tp,tn,fp,fn)\n",
    "    hss1 = HSS1(tp,tn,fp,fn)\n",
    "    hss2 = HSS2(tp,tn,fp,fn)\n",
    "    gss = GSS(tp,tn,fp,fn)\n",
    "    recall = Recall(tp,tn,fp,fn)\n",
    "    precision = Precision(tp,tn,fp,fn)\n",
    "    \n",
    "    output_values = np.array([tp, fn, fp, tn, tss, hss1, hss2, gss, recall, precision])\n",
    "\n",
    "\n",
    "    #joblib.dump(classifier, data_dir + \"svm_model.pkl\")\n",
    "\n",
    "    #loaded_svm_model = joblib.load(data_dir + \"svm_model.pkl\")\n",
    "    \n",
    "    return output_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76765c1c",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(reslut, name):\n",
    "    data_dir = \"/Users/samskanderi/Documents/Research_Project/SWANSF/code/results/\"\n",
    "\n",
    "    with open(data_dir + name + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(reslut, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2410b6",
   "metadata": {},
   "source": [
    "## 2D to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379eed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = 24\n",
    "num_partitions = 5\n",
    "num_timestamps = 60\n",
    "X_train_concat_ZM_3D = []\n",
    "for i in range(0, num_partitions):\n",
    "    new_3D = np.zeros((X_train_concat_ZM[i].shape[0], num_timestamps, num_attributes))\n",
    "\n",
    "    for j in range(0, X_train_concat_ZM[i].shape[0]):\n",
    "        for m in range(0, num_attributes):\n",
    "            new_3D[j,:,m] = X_train_concat_ZM[i][j,m*num_timestamps:(m+1)*num_timestamps]\n",
    "    X_train_concat_ZM_3D.append(new_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d669c94",
   "metadata": {},
   "source": [
    "### GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_withKNN = kfold_training('GRU', X_train_concat_ZM_3D, Y_train_concat_ZM, gru_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_withKNN, \"GRU_With_MVTS-KNNI_Concatenation_Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd086ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_withKNN = kfold_training('GRU', X_train_concat_ZM_3D, Y_train_concat_ZM, gru_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_withKNN, \"GRU_With_MVTS-KNNI_Concatenation_Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59820f",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_concat = kfold_training('GRU', X_train_concat_ZM_3D, Y_train_concat_ZM, gru_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(gru_concat, \"GRU_Concatenation_Results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
